{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6456bf37-ded5-4b91-a326-5968e005f7ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "228937785fae498da6b4587c47616230",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "from pickle import dump, load\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from keras.applications.xception import Xception, preprocess_input\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.layers.merge import add\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense, LSTM, Embedding, Dropout\n",
    "\n",
    "# small library for seeing the progress of loops.\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "tqdm().pandas()\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c993209-0130-4c02-b585-f5afaca1714e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_RAW = \"../data/raw\"\n",
    "DATASET_INTERIM = \"../data/interim\"\n",
    "CHECKPOINTS = \"../checkpoints\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d0cd40-d8d3-425a-b7ce-b262d3e5d1d3",
   "metadata": {},
   "source": [
    "## Load & clean captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de0e1c05-28b2-4565-a983-69e75841cc18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of captions: \t8091\n",
      "Length of vocab: \t8763\n"
     ]
    }
   ],
   "source": [
    "def clean_caption(caption, table):\n",
    "    caption.replace(\"-\", \" \")    # Replace \"-\" with \" \"\n",
    "    words = caption.split()      # Split the words\n",
    "    \n",
    "    words = [word.lower() for word in words]    # Convert to lowercase\n",
    "    words = [word.translate(table) for word in words]  # Remove punctuations\n",
    "    words = [word for word in words if(len(word)>1)]   # Remove 's and a\n",
    "    words = [word for word in words if(word.isalpha())] # Remove tokens with numbers\n",
    "    \n",
    "    return ' '.join(words)\n",
    "\n",
    "\n",
    "def load_captions(filename):\n",
    "    # Load the text file into memory\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    table = str.maketrans('','',string.punctuation)\n",
    "    \n",
    "    img_captions = dict()\n",
    "    \n",
    "    lines = text.split('\\n')\n",
    "    for line in lines[1:]:\n",
    "        if len(line) == 0:\n",
    "            continue\n",
    "        img, caption = line.split(',', 1)\n",
    "        # Clean the caption text\n",
    "        caption = clean_caption(caption, table)\n",
    "        if img in img_captions:\n",
    "            img_captions[img].append(caption)\n",
    "        else:\n",
    "            img_captions[img] = [caption]\n",
    "        \n",
    "    return img_captions\n",
    "\n",
    "\n",
    "def text_vocabulary(img_captions):\n",
    "    vocab = set()\n",
    "    for img in img_captions:\n",
    "        [vocab.update(d.split()) for d in img_captions[img]]\n",
    "\n",
    "    return vocab\n",
    "\n",
    "img_captions = load_captions(os.path.join(DATASET_RAW, 'captions.txt'))\n",
    "print(f\"Number of captions: \\t{len(img_captions)}\")\n",
    "\n",
    "vocabulary = text_vocabulary(img_captions)\n",
    "print(f\"Length of vocab: \\t{len(vocabulary)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc8236d0-8cf1-4f92-a45f-8027aaa8e2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_img_captions(img_captions, filename):\n",
    "    lines = list()\n",
    "    for key, desc_list in img_captions.items():\n",
    "        for desc in desc_list:\n",
    "            lines.append(key + '\\t' + desc )\n",
    "    data = \"\\n\".join(lines)\n",
    "    file = open(filename,\"w\")\n",
    "    file.write(data)\n",
    "    file.close()\n",
    "    \n",
    "\n",
    "save_img_captions(img_captions, os.path.join(DATASET_INTERIM, 'captions.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e368b52-345a-493a-afad-5ed9561e7197",
   "metadata": {},
   "source": [
    "## Extracting the features from images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0d071a6b-a405-4918-aeaa-51826ac89feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-04 20:07:09.421439: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-11-04 20:07:09.475971: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-11-04 20:07:09.477446: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-11-04 20:07:09.479969: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-04 20:07:09.488934: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-11-04 20:07:09.489908: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-11-04 20:07:09.490752: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-11-04 20:07:16.622065: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-11-04 20:07:16.622889: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-11-04 20:07:16.623583: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-11-04 20:07:16.624082: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9352 MB memory:  -> device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:00:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "83689472/83683744 [==============================] - 1s 0us/step\n",
      "83697664/83683744 [==============================] - 1s 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_703/3649821741.py:4: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for img in tqdm(os.listdir(directory)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88ddcc4b1a8b4215a9280bd7e882ee22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8091 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-04 20:07:18.987298: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2023-11-04 20:07:23.180721: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8201\n",
      "2023-11-04 20:07:27.142004: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "def extract_img_features(directory):\n",
    "    model = Xception(include_top=False, pooling='avg')\n",
    "    features = {}\n",
    "    for img in tqdm(os.listdir(directory)):\n",
    "        filename = directory + \"/\" + img\n",
    "        image = Image.open(filename)\n",
    "        image = image.resize((299,299))\n",
    "        image = np.expand_dims(image, axis=0)\n",
    "        image = image/127.5\n",
    "        image = image - 1.0\n",
    "\n",
    "        feature = model.predict(image)\n",
    "        features[img] = feature\n",
    "    return features\n",
    "\n",
    "#2048 feature vector\n",
    "img_features = extract_img_features(os.path.join(DATASET_RAW, 'Images'))\n",
    "dump(img_features, open(os.path.join(DATASET_INTERIM, 'img_features.pkl'), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e771dc3c-dcb7-456d-a9fe-1cf70082a7c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of images:\t8091\n",
      "(1, 2048)\n"
     ]
    }
   ],
   "source": [
    "img_features = load(open(os.path.join(DATASET_INTERIM, 'img_features.pkl'),\"rb\"))\n",
    "\n",
    "print(f\"# of images:\\t{len(img_features)}\")\n",
    "print(img_features['1000268201_693b08cb0e.jpg'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75014495-8c97-48eb-a2d4-225705f8b9b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8091"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(img_captions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245092ca-3d43-45ed-84b3-ccf69b39b738",
   "metadata": {},
   "source": [
    "## Split the training/dev/test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70d49120-5277-468b-81d5-3fd8f14d0ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 6000 | dev: 1000 | test: 1091\n"
     ]
    }
   ],
   "source": [
    "img_files = list(img_captions.keys())\n",
    "\n",
    "assert len(img_files) > 7000, \"The list must have more than 7000 elements.\"\n",
    "\n",
    "# Shuffle the list in place\n",
    "random.shuffle(img_files)\n",
    "\n",
    "# Split into training, dev, and test datasets\n",
    "train_imgs = img_files[:6000]\n",
    "dev_imgs = img_files[6000:7000]\n",
    "test_imgs = img_files[7000:]  # The remaining part of the list\n",
    "\n",
    "dataset = {\n",
    "    \"train\": train_imgs,\n",
    "    \"dev\": dev_imgs,\n",
    "    \"test\": test_imgs\n",
    "}\n",
    "\n",
    "print(f\"train: {len(train_imgs)} | dev: {len(dev_imgs)} | test: {len(test_imgs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d722219-3aa9-4d50-aa3b-f90d106166eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_captions(img_captions, mode=\"train\", dataset=dataset):\n",
    "    \"\"\"\n",
    "    Generate an img-captions dictionary with images from a specifc dataset\n",
    "    \n",
    "    Reture:\n",
    "        A dictionary (img filename: <start> caption <end>\n",
    "    \"\"\"\n",
    "    captions = dict()\n",
    "    for img_name in img_captions:\n",
    "        if img_name in dataset[mode]:\n",
    "            captions[img_name] = [\n",
    "                f\"<start> {caption} <end>\"\n",
    "                for caption in img_captions[img_name]\n",
    "            ]\n",
    "    \n",
    "    return captions\n",
    "            \n",
    "\n",
    "def get_img_features(img_features, mode=\"train\", dataset=dataset):\n",
    "    features = dict()\n",
    "    for img_name in img_features:\n",
    "        if img_name in dataset[mode]:\n",
    "            features[img_name] = img_features[img_name]\n",
    "    \n",
    "    return features\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3380618a-b488-496a-9450-69335bdb37cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_captions = img_captions\n",
    "all_img_features = img_features\n",
    "\n",
    "train_captions = get_captions(img_captions, \"train\", dataset)\n",
    "train_img_features = get_img_features(img_features, \"train\", dataset)\n",
    "\n",
    "dev_captions = get_captions(img_captions, \"dev\", dataset)\n",
    "dev_img_features = get_img_features(img_features, \"dev\", dataset)\n",
    "\n",
    "test_captions = get_captions(img_captions, \"test\", dataset)\n",
    "test_img_features = get_img_features(img_features, \"test\", dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5461ac0-0119-45f1-9262-6bbbff35657a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8091 6000 1000 1091\n",
      "8091 1000 1091\n"
     ]
    }
   ],
   "source": [
    "print(len(all_captions), len(train_captions), len(dev_captions), len(test_captions))\n",
    "print(len(all_img_features), len(dev_img_features), len(test_img_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f1e60f-36b6-4cfc-aefc-13c781576225",
   "metadata": {},
   "source": [
    "all_captionstrain_captions## Tokenize the training captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "081df910-407b-4f27-a236-e7ff9bd7f588",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_list(captions):\n",
    "    all_caption = []\n",
    "    for key in captions:\n",
    "        [all_caption.append(caption) for caption in captions[key]]\n",
    "    return all_caption\n",
    "\n",
    "\n",
    "def create_tokenizer(captions):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(dict_to_list(captions))\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ab89411-c74b-4183-9a23-568cff06de2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocab = 7687\n"
     ]
    }
   ],
   "source": [
    "tokenizer = create_tokenizer(train_captions)\n",
    "# Save the tokenize to hard drive\n",
    "dump(tokenizer, open(os.path.join(DATASET_INTERIM, 'tokenizer.pkl'), 'wb'))\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(f\"Size of vocab = {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0865feb9-88f7-4417-9b13-e57dc4f11736",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def max_length(captions):\n",
    "    desc_list = dict_to_list(captions)\n",
    "    return max(len(d.split()) for d in desc_list)\n",
    "    \n",
    "max_length = max_length(img_captions)\n",
    "max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6298b9bf-bb85-46eb-9f19-dd34fc00d2d1",
   "metadata": {},
   "source": [
    "## Create data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ec6aa95b-bac6-4232-a6fd-bd5e01e85ad3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((47, 2048), (47, 32), (47, 7687))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create input-output sequence pairs from the image description.\n",
    "\n",
    "# data generator, used by model.fit_generator()\n",
    "def data_generator(descriptions, features, tokenizer, max_length=32):\n",
    "    while True:\n",
    "        for key, description_list in descriptions.items():\n",
    "            feature = features[key][0]\n",
    "            input_image, input_sequence, output_word = create_sequences(tokenizer, max_length, description_list, feature)\n",
    "            yield [input_image, input_sequence], output_word\n",
    "\n",
    "            \n",
    "def create_sequences(tokenizer, max_length, desc_list, feature):\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    # walk through each description for the image\n",
    "    for desc in desc_list:\n",
    "        # encode the sequence\n",
    "        seq = tokenizer.texts_to_sequences([desc])[0]\n",
    "        # split one sequence into multiple X,y pairs\n",
    "        for i in range(1, len(seq)):\n",
    "            # split into input and output pair\n",
    "            in_seq, out_seq = seq[:i], seq[i]\n",
    "            # pad input sequence\n",
    "            in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "            # encode output sequence\n",
    "            out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "            # store\n",
    "            X1.append(feature)\n",
    "            X2.append(in_seq)\n",
    "            y.append(out_seq)\n",
    "    return np.array(X1), np.array(X2), np.array(y)\n",
    "\n",
    "# You can check the shape of the input and output for your model\n",
    "[a, b], c = next(data_generator(train_captions, train_img_features, tokenizer, max_length))\n",
    "a.shape, b.shape, c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d74f9c8a-d786-4eae-afb8-8e409f63b376",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(vocab_size, max_length):\n",
    "\n",
    "    # features from the CNN model squeezed from 2048 to 256 nodes\n",
    "    inputs1 = Input(shape=(2048,))\n",
    "    fe1 = Dropout(0.5)(inputs1)\n",
    "    fe2 = Dense(256, activation='relu')(fe1)\n",
    "\n",
    "    # LSTM sequence model\n",
    "    inputs2 = Input(shape=(max_length,))\n",
    "    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "    se2 = Dropout(0.5)(se1)\n",
    "    se3 = LSTM(256)(se2)\n",
    "\n",
    "    # Merging both models\n",
    "    decoder1 = add([fe2, se3])\n",
    "    decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "    outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "\n",
    "    # tie it together [image, seq] [word]\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "    # summarize model\n",
    "    # print(model.summary())\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9dd170b-3b89-469a-b2a3-1ad5a3b37159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset:  6000\n",
      "Descriptions: train = 6000\n",
      "Photos: train = 6000\n",
      "Vocabulary Size: 7687\n",
      "Description Length:  32\n",
      "5021/6000 [========================>.....] - ETA: 13:46 - loss: 4.6248"
     ]
    }
   ],
   "source": [
    "# train our model\n",
    "print('Dataset: ', len(train_imgs))\n",
    "print('Descriptions: train =', len(train_captions))\n",
    "print('Photos: train =', len(train_img_features))\n",
    "print('Vocabulary Size:', vocab_size)\n",
    "print('Description Length: ', max_length)\n",
    "\n",
    "model = define_model(vocab_size, max_length)\n",
    "epochs = 10\n",
    "steps = len(train_captions)\n",
    "# making a directory models to save our models\n",
    "for i in range(epochs):\n",
    "    generator = data_generator(train_captions, train_img_features, tokenizer, max_length)\n",
    "    model.fit(x=generator, epochs=1, steps_per_epoch=steps, verbose=1)\n",
    "    model.save(os.path.join(CHECKPOINTS, f\"/model_{str(i)}.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07015473-3d50-4315-bf56-66bf1aa4cb9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
